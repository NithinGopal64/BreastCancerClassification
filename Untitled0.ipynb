{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Untitled0.ipynb","provenance":[],"authorship_tag":"ABX9TyNr5M4fwqycZVBjJvU1bgjU"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9-3WXd0I1F62","executionInfo":{"status":"ok","timestamp":1654331332535,"user_tz":-330,"elapsed":919,"user":{"displayName":"UG SSN2022","userId":"10199486142267826046"}},"outputId":"8529f588-291e-463b-b5fc-ff3dc9fa71b3"},"outputs":[{"output_type":"stream","name":"stdout","text":["Cloning into 'Image-Classification-and-Localization-using-Multiple-Instance-Learning'...\n","remote: Enumerating objects: 104, done.\u001b[K\n","remote: Counting objects: 100% (104/104), done.\u001b[K\n","remote: Compressing objects: 100% (79/79), done.\u001b[K\n","remote: Total 104 (delta 39), reused 89 (delta 24), pack-reused 0\u001b[K\n","Receiving objects: 100% (104/104), 629.68 KiB | 13.40 MiB/s, done.\n","Resolving deltas: 100% (39/39), done.\n"]}],"source":["!git clone https://github.com/Dipeshtamboli/Image-Classification-and-Localization-using-Multiple-Instance-Learning.git"]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","\n","class Attention(nn.Module):\n","    def __init__(self):\n","        super(Attention, self).__init__()\n","        self.L = 500\n","        self.D = 128\n","        self.K = 1\n","\n","        self.feature_extractor_part1 = nn.Sequential(\n","            nn.Conv2d(3, 20, kernel_size=5),\n","            nn.ReLU(),\n","            nn.MaxPool2d(2, stride=2),\n","            nn.Conv2d(20, 50, kernel_size=5),\n","            nn.ReLU(),\n","            nn.MaxPool2d(2, stride=2)\n","        )\n","\n","        self.feature_extractor_part2 = nn.Sequential(\n","            nn.Linear(50 * 4 * 4, self.L),\n","            nn.ReLU(),\n","        )\n","\n","        self.attention = nn.Sequential(\n","            nn.Linear(self.L, self.D),\n","            nn.Tanh(),\n","            nn.Linear(self.D, self.K)\n","        )\n","\n","        self.classifier = nn.Sequential(\n","            nn.Linear(self.L*self.K, 1),\n","            nn.Sigmoid()\n","        )\n","\n","    def forward(self, x):\n","        # print('========================')\n","        # print(x.shape)\n","        x = x.squeeze(0)\n","        # print(x.shape)\n","        # print('========================')\n","        H = self.feature_extractor_part1(x)\n","        H = H.view(-1, 50 * 4 * 4)\n","        H = self.feature_extractor_part2(H)  # NxL\n","\n","        A = self.attention(H)  # NxK\n","        A = torch.transpose(A, 1, 0)  # KxN\n","        A = F.softmax(A, dim=1)  # softmax over N\n","\n","        M = torch.mm(A, H)  # KxL\n","\n","        Y_prob = self.classifier(M)\n","        Y_hat = torch.ge(Y_prob, 0.5).float()\n","\n","        return Y_prob, Y_hat, A\n","\n","    # AUXILIARY METHODS\n","    def calculate_classification_error(self, X, Y):\n","        Y = Y.float()\n","        _, Y_hat, _ = self.forward(X)\n","        # error = 1. - Y_hat.eq(Y).cpu().float().mean().data[0]\n","        error = 1. - Y_hat.eq(Y).cpu().float().mean().item()\n","\n","        return error, Y_hat\n","\n","    def calculate_objective(self, X, Y):\n","        Y = Y.float()\n","        Y_prob, _, A = self.forward(X)\n","        Y_prob = torch.clamp(Y_prob, min=1e-5, max=1. - 1e-5)\n","        neg_log_likelihood = -1. * (Y * torch.log(Y_prob) + (1. - Y) * torch.log(1. - Y_prob))  # negative log bernoulli\n","\n","        return neg_log_likelihood, A"],"metadata":{"id":"Jowx5vwD1Lao","executionInfo":{"status":"ok","timestamp":1654331406828,"user_tz":-330,"elapsed":3985,"user":{"displayName":"UG SSN2022","userId":"10199486142267826046"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["import os\n","import glob\n","from PIL import Image\n","import numpy as np\n","import torch\n","from torchvision import datasets, transforms\n","import random\n","# dir structure would be: data/class_name(0 and 1)/dir_containing_img/img\n","class PatchMethod(torch.utils.data.Dataset):\n","    def __init__(self, root = 'Desktop/screenshots/', mode = 'train', transform = None):\n","        self.root = root\n","        self.mode = mode\n","        self.raw_samples = glob.glob(root + '/*/*')\n","        # print(self.raw_samples)\n","        self.samples = []\n","        for raw_sample in self.raw_samples:\n","            self.samples.append((raw_sample, int(raw_sample.split('/')[-2])))\n","            # print(raw_sample,int(raw_sample.split('/')[-2]))\n","        # print(self.samples)\n","    \n","    def __len__(self):\n","        return len(self.samples)\n","    \n","    def __getitem__(self, index):\n","        if self.mode == 'train':\n","            random.shuffle(self.samples)\n","            \n","        image_dir, label = self.samples[index]\n","        images = glob.glob(image_dir)\n","        # images = glob.glob(image_dir + '/*')\n","        \n","        t = transforms.Compose([transforms.CenterCrop((448,700))]) #centercropping to 1200 to generate 9 400x400 patches\n","        \n","        transformations = transforms.Compose([\n","            transforms.ToTensor()\n","        ])\n","        \n","        array = []\n","        \n","        for i, image_path in enumerate(images):\n","            # print(image_path)\n","            image = Image.open(image_path)\n","            image = np.array(t(image))\n","            # print(image.shape)\n","            # image = np.array(image)\n","            r, c, _ = image.shape\n","            # print(\"image.shape\",image.shape)\n","            for i in range(0,28*16,28):\n","                for j in range(0,28*25,28):\n","                    array.append(transformations(image[i:i+28, j:j+28, :]))\n","#                     array.append(transformations(image[i:i+400, j:j+400, :]).float())\n","#                     array.append(image[i:i+400, j:j+400, :])\n","                    # if (i+400 < r) and (j+400 < c):\n","                        # array.append(transformations(image[i:i+400, j:j+400, :]).float())\n","                        # array.append(image[i:i+400, j:j+400, :])\n","                    \n","                    \n","        array = tuple(array)\n","        # print(\"################### array ###################\")\n","        # print(array)\n","        array = torch.stack(array, 0)\n","        \n","        return (array, label)"],"metadata":{"id":"iPscjd9o1eVf","executionInfo":{"status":"ok","timestamp":1654331438705,"user_tz":-330,"elapsed":396,"user":{"displayName":"UG SSN2022","userId":"10199486142267826046"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["!cp ./Image-Classification-and-Localization-using-Multiple-Instance-Learning/AMIL_codes/* ."],"metadata":{"id":"9pfamXJ92BOo","executionInfo":{"status":"ok","timestamp":1654331570651,"user_tz":-330,"elapsed":326,"user":{"displayName":"UG SSN2022","userId":"10199486142267826046"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["from __future__ import print_function\n","\n","import numpy as np\n","import imageio\n","import argparse\n","import torch\n","import torch.utils.data as data_utils\n","import torch.optim as optim\n","from torch.autograd import Variable\n","\n","# from dataloader import MnistBags\n","from amil_model import Attention\n","\n","# from __future__ import print_function, division\n","import os\n","import glob\n","from PIL import Image\n","import numpy as np\n","from torchvision import datasets, transforms\n","from torchvision import models\n","import random\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","\n","from amil_model import Attention\n","from patch_data import PatchMethod\n","from tensorboardX import SummaryWriter\n","import argparse\n","lparser = argparse.ArgumentParser(description='Breakthis data_mynet')\n","# parser.add_argument(\"--zoom\", help='zoom_level',default=400)\n","# parser.add_argument('--epochs',type=int, default=1, metavar='N',\n","#                     help='number of epochs to train (default: 10)')\n","# parser.add_argument('--seed', type=int, default=1, metavar='S',\n","#                     help='random seed (default: 1)')\n","# parser.add_argument('--no-cuda', action='store_true', default=False,\n","#                     help='disables CUDA training')\n","# parser.add_argument('--lr', type=float, default=0.0001, metavar='LR',\n","#                     help='learning rate (default: 0.01)')\n","# parser.add_argument('--reg', type=float, default=10e-5, metavar='R',\n","#                     help='weight decay')\n","\n","args = parser.parse_args()\n","print(args)\n","exit()\n","args.cuda = not args.no_cuda and torch.cuda.is_available()\n","\n","zoom_level_x =str(args.zoom) + 'X'\n","\n","\n","print('zoom_level_{} epoch_{} learning_rate_{}'.format(zoom_level_x, args.epochs, args.lr))\n","writer = SummaryWriter(zoom_level_x+'/runs/'+\"epoch:\"+str(args.epochs))\n","\n","# Training settings\n","# parser = argparse.ArgumentParser(description='PyTorch MNIST bags Example')\n","# parser.add_argument('--epochs', type=int, default=1, metavar='N',\n","#                     help='number of epochs to train (default: 10)')\n","# parser.add_argument('--lr', type=float, default=0.0001, metavar='LR',\n","#                     help='learning rate (default: 0.01)')\n","# parser.add_argument('--reg', type=float, default=10e-5, metavar='R',\n","#                     help='weight decay')\n","# parser.add_argument('--target_number', type=int, default=9, metavar='T',\n","#                     help='bags have a positive labels if they contain at least one 9')\n","# parser.add_argument('--mean_bag_length', type=int, default=10, metavar='ML',\n","#                     help='average bag length')\n","# parser.add_argument('--var_bag_length', type=int, default=2, metavar='VL',\n","#                     help='variance of bag length')\n","# parser.add_argument('--num_bags_train', type=int, default=200, metavar='NTrain',\n","#                     help='number of bags in training set')\n","# parser.add_argument('--num_bags_test', type=int, default=50, metavar='NTest',\n","#                     help='number of bags in test set')\n","# parser.add_argument('--seed', type=int, default=1, metavar='S',\n","#                     help='random seed (default: 1)')\n","# parser.add_argument('--no-cuda', action='store_true', default=False,\n","#                     help='disables CUDA training')\n","\n","# args = parser.parse_args()\n","# args.cuda = not args.no_cuda and torch.cuda.is_available()\n","\n","torch.manual_seed(args.seed)\n","if args.cuda:\n","    torch.cuda.manual_seed(args.seed)\n","    print('\\nGPU is ON!')\n","\n","print('Load Train and Test Set')\n","loader_kwargs = {'num_workers': 1, 'pin_memory': True} if args.cuda else {}\n","\n","# train_loader = data_utils.DataLoader(MnistBags(target_number=args.target_number,\n","#                                                mean_bag_length=args.mean_bag_length,\n","#                                                var_bag_length=args.var_bag_length,\n","#                                                num_bag=args.num_bags_train,\n","#                                                seed=args.seed,\n","#                                                train=True),\n","#                                      batch_size=1,\n","#                                      shuffle=True,\n","#                                      **loader_kwargs)\n","\n","# test_loader = data_utils.DataLoader(MnistBags(target_number=args.target_number,\n","#                                               mean_bag_length=args.mean_bag_length,\n","#                                               var_bag_length=args.var_bag_length,\n","#                                               num_bag=args.num_bags_test,\n","#                                               seed=args.seed,\n","#                                               train=False),\n","#                                     batch_size=1,\n","#                                     shuffle=False,\n","#                                     **loader_kwargs)\n","\n","# dir structure would be: data/class_name(0 and 1)/dir_containing_img/img\n","# sftp://test1@10.107.42.42/home/Drive2/amil/data\n","# /home/dipesh/test1/new_data_tree/40X\n","# /home/dipesh/126/AMIL_project/AMIL_codes/amil_model.py\n","# /home/dipesh/126/AMIL_project/\n","data_path_train = \"../AMIL_Data/{}/train\".format(zoom_level_x)\n","data_path_test = \"../AMIL_Data/{}/test\".format(zoom_level_x)\n","\n","data = PatchMethod(root = data_path_train)\n","val_data =PatchMethod(root = data_path_test, mode = 'test')\n","# data = PatchMethod(root = '/Users/abhijeetpatil/Desktop/screenshots2/')\n","# val_data =PatchMethod(root = '/Users/abhijeetpatil/Desktop/screenshots2/', mode = 'test')\n","\n","train_loader = torch.utils.data.DataLoader(data, shuffle = True, num_workers = 6, batch_size = 1)\n","test_loader = torch.utils.data.DataLoader(val_data, shuffle = False, num_workers = 6, batch_size = 1)\n","\n","\n","print('Init Model')\n","model = Attention()\n","if args.cuda:\n","    model.cuda()\n","\n","optimizer = optim.Adam(model.parameters(), lr=args.lr, betas=(0.9, 0.999), weight_decay=args.reg)\n","# optimizer = optim.Adam(model.parameters(), lr=0.001)\n","\n","\n","def train(epoch):\n","    model.train()\n","    train_loss = 0.\n","    train_error = 0.\n","    correct_label_pred = 0\n","    for batch_idx, (data, label) in enumerate(train_loader):\n","        # print(\"label\",label[0][0])\n","        # print(\"label\",label[0])\n","        bag_label = label[0]\n","        if args.cuda:\n","            data, bag_label = data.cuda(), bag_label.cuda()\n","        data, bag_label = Variable(data), Variable(bag_label)\n","        data = data.squeeze(0)\n","\n","        optimizer.zero_grad()\n","        # calculate loss and metrics\n","        loss, _ = model.calculate_objective(data, bag_label)\n","        train_loss += loss.data[0]\n","        error, predicted_label_train = model.calculate_classification_error(data, bag_label)\n","        # print(\"bag_label,predicted_label_train\",bag_label,predicted_label_train)\n","        # print(int(bag_label) == int(predicted_label_train))\n","        correct_label_pred += (int(bag_label) == int(predicted_label_train))\n","        # exit()\n","        train_error += error\n","        # backward pass\n","        loss.backward()\n","        # step\n","        optimizer.step()\n","        # print(correct_label_pred)\n","        # print(len(train_loader))\n","        # print(batch_idx)\n","\n","    # calculate loss and error for epoch\n","    train_loss /= len(train_loader)\n","    train_error /= len(train_loader)\n","    train_acc = (1 - train_error)*100\n","\n","    writer.add_scalar('data/train_acc', train_acc, epoch)\n","    writer.add_scalar('data/train_error', train_error, epoch)\n","    writer.add_scalar('data/train_loss', train_loss, epoch)\n","\n","    result_train = 'Epoch: {}, Loss: {:.4f}, Train error: {:.4f}, Train accuracy: {:.2f}'.format(epoch, train_loss.cpu().numpy()[0], train_error, train_acc)\n","\n","    print(result_train)\n","    return result_train\n","\n","def test(epoch):\n","    model.eval()\n","    test_loss = 0.\n","    test_error = 0.\n","    for batch_idx, (data, label) in enumerate(test_loader):\n","        # print(label)\n","        # print((data[0].shape))\n","\n","        bag_label = label[0]\n","        instance_labels = label[0]\n","        if args.cuda:\n","            data, bag_label = data.cuda(), bag_label.cuda()\n","        data, bag_label = Variable(data), Variable(bag_label)\n","        loss, attention_weights = model.calculate_objective(data, bag_label)\n","        test_loss += loss.data[0]\n","        error, predicted_label = model.calculate_classification_error(data, bag_label)\n","        test_error += error\n","\n","        visualization_attention(data[0],attention_weights[0],batch_idx,epoch)\n","        if batch_idx < 1:  # plot bag labels and instance labels for first 5 bags\n","            bag_level = (bag_label.cpu().data.numpy(), int(predicted_label.cpu().data.numpy()))\n","            # print(bag_level)\n","            # print(instance_labels)\n","            # visualization_attention(data[0],attention_weights[0],batch_idx,epoch)\n","            # print(\"attention_weights.shape\",attention_weights.shape)\n","            # instance_level = list(zip(instance_labels.numpy().tolist(),\n","            #                      np.round(attention_weights.cpu().data.numpy(), decimals=3).tolist()))\n","\n","            # print('\\nTrue Bag Label, Predicted Bag Label: {}\\n'\n","            #       'True Instance Labels, Attention Weights: {}'.format(bag_level, instance_level))\n","\n","    test_error /= len(test_loader)\n","    test_loss /= len(test_loader)\n","    test_acc = (1 - test_error)*100    \n","\n","    writer.add_scalar('data/test_acc', test_acc, epoch)\n","    writer.add_scalar('data/test_error', test_error, epoch)\n","    writer.add_scalar('data/test_loss', test_loss, epoch)\n","    result_test = 'Epoch: {}, Loss: {:.4f}, test error: {:.4f}, test accuracy: {:.2f}'.format(epoch, test_loss.cpu().numpy()[0], test_error, test_acc)\n","    print(result_test)\n","    return result_test\n","    # print('Test Set, Loss: {:.4f}, Test error: {:.4f}'.format(test_loss.cpu().numpy()[0], test_error))\n","\n","def visualization_attention(data,attention_weights,batch_idx,epoch):\n","    img_save_dir = './{}/AMIL_visualization/epoch_{}'.format(zoom_level_x,epoch)\n","    img_save_name = img_save_dir + '/test_epoch_{}_no_{}.png'.format(epoch,batch_idx)\n","    if not os.path.exists(img_save_dir):\n","        os.makedirs(img_save_dir)\n","\n","    data = data.cpu().data.numpy()\n","    attention_weights = attention_weights.cpu().data.numpy()\n","    # print(\"data.shape\",data.shape)\n","    # print(\"attention_weights\",attention_weights.shape)\n","    attention_weights = attention_weights/np.max(attention_weights)\n","    complete_image=np.zeros((3,480,700))\n","    for height_no in range(16):\n","        for width_no in range(25):\n","            complete_image[:,height_no*28:height_no*28+28, width_no*28:width_no*28+28] = data[height_no*25+width_no,:,:,:] * attention_weights[height_no*25+width_no]\n","    complete_image = complete_image.transpose((1,2,0))\n","    imwrite(img_save_name,complete_image)\n","    # weighted_images_list = data * attention_weights\n","\n","\n","\n","if __name__ == \"__main__\":\n","    # img_save_dir = './AMIL_visualization/zoom_{}/epoch_{}'.format(zoom_level_x,epoch)\n","    main_dir = \"./\" + zoom_level_x +'/'\n","    folders = [\"pt_files\",\"txt_file\"]\n","    for i in folders:\n","        if not os.path.exists(main_dir + i ):\n","            os.makedirs(main_dir + i )\n","\n","    save_string=\"AMIL_Breakthis_epochs: \"+str(args.epochs)+\"zoom:\"+zoom_level_x\n","    save_name_txt = main_dir+\"txt_file/\"+save_string+\".txt\"\n","\n","    model_file = open(save_name_txt,\"w\") \n","    for epoch in range(1, args.epochs + 1):\n","        print('----------Start Training----------')\n","        train_result = train(epoch)\n","        print('----------Start Testing----------')\n","        test_result = test(epoch)\n","        model_file.write(test_result + '\\n')\n","        model_file.write(train_result + '\\n')\n","    model_file.close()\n","    torch.save(model.state_dict(),main_dir+\"pt_files/\"+save_string+\"AMIL_Breakthis_state_dict.pt\")\n","    torch.save(model ,main_dir+\"pt_files/\"+save_string+\"AMIL_Breakthis_model.pt\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":191},"id":"UfRwNn831koH","executionInfo":{"status":"error","timestamp":1654331853057,"user_tz":-330,"elapsed":418,"user":{"displayName":"UG SSN2022","userId":"10199486142267826046"}},"outputId":"f5133085-bd14-4eb4-f029-525d9b5d8322"},"execution_count":21,"outputs":[{"output_type":"stream","name":"stderr","text":["usage: ipykernel_launcher.py [-h]\n","ipykernel_launcher.py: error: unrecognized arguments: -f /root/.local/share/jupyter/runtime/kernel-34cb32d1-b648-45d7-ba1a-72af729b0a89.json\n"]},{"output_type":"error","ename":"SystemExit","evalue":"ignored","traceback":["An exception has occurred, use %tb to see the full traceback.\n","\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py:2890: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n","  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"]}]},{"cell_type":"code","source":["! python train_n_test.py"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Qaqt35GG2ewv","executionInfo":{"status":"ok","timestamp":1654331679118,"user_tz":-330,"elapsed":918,"user":{"displayName":"UG SSN2022","userId":"10199486142267826046"}},"outputId":"e7188831-0a9d-4d92-e44b-79625261ae97"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["Traceback (most recent call last):\n","  File \"train_n_test.py\", line 4, in <module>\n","    from scipy.misc import imsave\n","ImportError: cannot import name 'imsave' from 'scipy.misc' (/usr/local/lib/python3.7/dist-packages/scipy/misc/__init__.py)\n"]}]}]}